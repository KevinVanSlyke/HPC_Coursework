#!/bin/sh
#SBATCH --partition=general-compute
#SBATCH --time=06:00:00
#SBATCH --nodes=4
#SBATCH --ntasks-per-node=8
#SBATCH --constraint=IB
#SBATCH --mem=24000
# Memory per node specification is in MB. It is optional. 
# The default limit is 3GB per core.
#SBATCH --job-name="hw3p1"
#SBATCH --output=hw3p1.out
#SBATCH --mail-user=kgvansly@buffalo.edu
#SBATCH --mail-type=ALL
#SBATCH --requeue
#Specifies that the job will be requeued after a node failure.
#The default is that the job will not be requeued.

echo "SLURM_JOBID="$SLURM_JOBID
echo "SLURM_JOB_NODELIST"=$SLURM_JOB_NODELIST
echo "SLURM_NNODES"=$SLURM_NNODES
echo "SLURMTMPDIR="$SLURMTMPDIR
echo "Working directory = "$SLURM_SUBMIT_DIR

module load intel
module load intel-mpi
module list
ulimit -s unlimited
#

make hw3p1

# The initial srun will trigger the SLURM prologue on the compute nodes.
NPROCS=`srun --nodes=${SLURM_NNODES} bash -c 'hostname' |wc -l`
echo NPROCS=$NPROCS
echo "Launch hw3p1 with srun"
#The PMI library is necessary for srun
export I_MPI_PMI_LIBRARY=/usr/lib64/libpmi.so

echo "Doing Problem 1 with:"

for ((i=1;i<=${NPROCS};i++))
do
	echo "$i Processor(s)"
	srun -n $i ./hw3p1
done

#
echo "All Done!"
